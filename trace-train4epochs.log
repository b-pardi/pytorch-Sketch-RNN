    CALL: hparam_parser in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\hparams.py']:8
    Arguments: 

    RETURN: ArgumentParser(prog='trace_train.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=False)

    CALL: train_sketch_rnn in sketchrnn-pytorch\train_sketch_rnn.py:59
    Arguments: args=Namespace(max_seq_len=250, enc_model='lstm', dec_model='layer_norm', enc_rnn_size=256, dec_rnn_size=512, z_size=128, num_mixture=20, r_dropout=0.1, kl_weight=0.5, kl_weight_start=0.01, kl_tolerance=0.2, kl_decay_rate=0.99995, reg_covar=1e-06, batch_size=100, lr=0.001, lr_decay=0.9999, min_lr=1e-05, grad_clip=1.0, data_set='cat.npz', random_scale_factor=0.15, augment_stroke_prob=0.1, data_dir='data/', save_dir=None, num_epochs=4, num_workers=8)

        CALL: load_strokes in sketchrnn-pytorch\sketch_rnn\dataset.py:15
        Arguments: data_dir='data/', hps=Namespace(max_seq_len=250, enc_model='lstm', dec_model='layer_norm', enc_rnn_size=256, dec_rnn_size=512, z_size=128, num_mixture=20, r_dropout=0.1, kl_weight=0.5, kl_weight_start=0.01, kl_tolerance=0.2, kl_decay_rate=0.99995, reg_covar=1e-06, batch_size=100, lr=0.001, lr_decay=0.9999, min_lr=1e-05, grad_clip=1.0, data_set='cat.npz', random_scale_factor=0.15, augment_stroke_prob=0.1, data_dir='data/', save_dir=None, num_epochs=4, num_workers=8)

            CALL: get_max_len in sketchrnn-pytorch\sketch_rnn\utils\misc.py:8
            Arguments: strokes=array([array([[ -3,  -8,   0],
              [  2, -38,   0],
              [ 14,   8,   0],
              ...,
              [ -3,  12,   0],
              [  0,  66,   0],
              [  3,  21,   1]], dtype=int16),
       array([[ -8, -41,   0],
              [-22, -51,   0],
              [ 17,  14,   0],
              ...,
              [-62, -80,   0],
              [-20, -34,   0],
              [ -6, -18,   1]], dtype=int16),
       array([[-40,   0,   0],
              [-63,  39,   0]... (truncated)

            RETURN: 129

        RETURN: (array([array([[ -3,  -8,   0],
              [  2, -38,   0],
              [ 14,   8,   0],
              ...,
              [ -3,  12,   0],
              [  0,  66,   0],
              [  3,  21,   1]], dtype=int16),
       array([[ -8, -41,   0],
              [-22, -51,   0],
              [ 17,  14,   0],
              ...,
              [-62, -80,   0],
              [-20, -34,   0],
              [ -6, -18,   1]], dtype=int16),
       array([[-40,   0,   0],
              [-63,  39,   0... (truncated)

        CALL: __init__ in sketchrnn-pytorch\sketch_rnn\dataset.py:69
        Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>, strokes=array([array([[ -3,  -8,   0],
              [  2, -38,   0],
              [ 14,   8,   0],
              ...,
              [ -3,  12,   0],
              [  0,  66,   0],
              [  3,  21,   1]], dtype=int16),
       array([[ -8, -41,   0],
              [-22, -51,   0],
              [ 17,  14,   0],
              ...,
              [-62, -80,   0],
              [-20, -34,   0],
              [ -6, -18,   1]], dtype=int16),
       array([[-40,   0,   0],
              [-63,  39,   0]... (truncated), max_len=129, scale_factor=None, random_scale_factor=0.15, augment_stroke_prob=0.1, limit=1000

            CALL: <listcomp> in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\dataset.py']:76
            Arguments: .0=<iterator object at 0x000002DD601C2500>

                CALL: to_tensor in sketchrnn-pytorch\sketch_rnn\utils\misc.py:17
                Arguments: x=array([[ -3,  -8,   0],
       [  2, -38,   0],
       [ 14,   8,   0],
       ...,
       [ -3,  12,   0],
       [  0,  66,   0],
       [  3,  21,   1]], dtype=int16)

                RETURN: Tensor(shape=(96, 3), dtype=torch.float32, device=cpu)

                CALL: to_tensor in sketchrnn-pytorch\sketch_rnn\utils\misc.py:17
                Arguments: x=array([[ -8, -41,   0],
       [-22, -51,   0],
       [ 17,  14,   0],
       ...,
       [-62, -80,   0],
       [-20, -34,   0],
       [ -6, -18,   1]], dtype=int16)

                RETURN: Tensor(shape=(81, 3), dtype=torch.float32, device=cpu)

                CALL: to_tensor in sketchrnn-pytorch\sketch_rnn\utils\misc.py:17
                Arguments: x=array([[-40,   0,   0],
       [-63,  39,   0],
       [-28,  26,   0],
       ...,
       [ 44, -10,   0],
       [ 26,  -1,   0],
       [  5, -19,   1]], dtype=int16)

                RETURN: Tensor(shape=(76, 3), dtype=torch.float32, device=cpu)

            RETURN: [tensor([[  -3.,   -8.,    0.],
        [   2.,  -38.,    0.],
        [  14.,    8.,    0.],
        [  20.,   26.,    0.],
        [  22.,  -14.,    0.],
        [  10.,   -3.,    0.],
        [  51.,    0.,    0.],
        [  31.,   -5.,    0.],
        [  12.,    3.,    0.],
        [   5.,   -6.,    0.],
        [   7.,  -21.,    0.],
        [   5.,   -3.,    0.],
        [   9.,   31.,    0.],
        [   0.,   91.,    0.],
        [  -2.,    8.,    0.],
        [ -12.,   11.,    0.],
   ... (truncated)

            CALL: preprocess in sketchrnn-pytorch\sketch_rnn\dataset.py:84
            Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>, strokes=[tensor([[  -3.,   -8.,    0.],
        [   2.,  -38.,    0.],
        [  14.,    8.,    0.],
        [  20.,   26.,    0.],
        [  22.,  -14.,    0.],
        [  10.,   -3.,    0.],
        [  51.,    0.,    0.],
        [  31.,   -5.,    0.],
        [  12.,    3.,    0.],
        [   5.,   -6.,    0.],
        [   7.,  -21.,    0.],
        [   5.,   -3.,    0.],
        [   9.,   31.,    0.],
        [   0.,   91.,    0.],
        [  -2.,    8.,    0.],
        [ -12.,   11.,    0.],
   ... (truncated)

                CALL: <listcomp> in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\dataset.py']:99
                Arguments: .0=<iterator object at 0x000002DD5D1D3160>

                RETURN: [tensor([[  7.,  -5.,   0.],
        [  6.,   0.,   0.],
        [ 30.,  28.,   0.],
        [  4.,  10.,   0.],
        [  1.,  27.,   0.],
        [ -2.,  17.,   0.],
        [ -7.,  13.,   0.],
        [-20.,  21.,   0.],
        [-20.,   9.,   0.],
        [-42.,   2.,   0.],
        [-16.,  -3.,   0.],
        [-22., -14.,   0.],
        [ -6.,  -9.,   0.],
        [ -1., -33.,   0.],
        [ 16., -29.,   0.],
        [ 26., -19.,   0.],
        [ 28., -10.,   1.],
        [-50.,  16.,   ... (truncated)

            RETURN: None

            CALL: normalize in sketchrnn-pytorch\sketch_rnn\dataset.py:108
            Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>, scale_factor=None

                CALL: calculate_normalizing_scale_factor in sketchrnn-pytorch\sketch_rnn\dataset.py:102
                Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>

                    CALL: <listcomp> in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\dataset.py']:104
                    Arguments: .0=<list_iterator object at 0x000002DD5D1D37C0>

                    RETURN: [tensor([[  7.,  -5.,   0.],
        [  6.,   0.,   0.],
        [ 30.,  28.,   0.],
        [  4.,  10.,   0.],
        [  1.,  27.,   0.],
        [ -2.,  17.,   0.],
        [ -7.,  13.,   0.],
        [-20.,  21.,   0.],
        [-20.,   9.,   0.],
        [-42.,   2.,   0.],
        [-16.,  -3.,   0.],
        [-22., -14.,   0.],
        [ -6.,  -9.,   0.],
        [ -1., -33.,   0.],
        [ 16., -29.,   0.],
        [ 26., -19.,   0.],
        [ 28., -10.,   1.],
        [-50.,  16.,   ... (truncated)

                RETURN: Tensor(shape=(), dtype=torch.float32, device=cpu)

            RETURN: None

        RETURN: None

        CALL: __init__ in sketchrnn-pytorch\sketch_rnn\dataset.py:69
        Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD5D1D31F0>, strokes=array([array([[ 23, -12,   0],
              [ 29, -26,   0],
              [ 11, -15,   0],
              ...,
              [ 11, -26,   0],
              [  9,  -1,   0],
              [ 13, -10,   1]], dtype=int16),
       array([[-18,  -1,   0],
              [-18,  -7,   0],
              [-57,  -9,   0],
              ...,
              [  5,   1,   0],
              [ 36,  38,   0],
              [ 60,  90,   1]], dtype=int16),
       array([[ 37, -66,   0],
              [ 17, -38,   0]... (truncated), max_len=129, scale_factor=Tensor(shape=(), dtype=torch.float32, device=cpu), random_scale_factor=0.0, augment_stroke_prob=0.0, limit=1000

            CALL: <listcomp> in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\dataset.py']:76
            Arguments: .0=<iterator object at 0x000002DD5D1D3FD0>

            RETURN: [tensor([[  23.,  -12.,    0.],
        [  29.,  -26.,    0.],
        [  11.,  -15.,    0.],
        [  16.,  -48.,    0.],
        [  23.,  -42.,    0.],
        [  27.,  -22.,    0.],
        [  26.,  -11.,    0.],
        [  30.,   -8.,    0.],
        [  22.,   -2.,    0.],
        [  10.,    0.,    0.],
        [   1.,    5.,    0.],
        [ -53.,   26.,    0.],
        [ -36.,   44.,    0.],
        [ -16.,   35.,    0.],
        [ -43.,   66.,    0.],
        [ -51.,   34.,    0.],
   ... (truncated)

            CALL: preprocess in sketchrnn-pytorch\sketch_rnn\dataset.py:84
            Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD5D1D31F0>, strokes=[tensor([[  23.,  -12.,    0.],
        [  29.,  -26.,    0.],
        [  11.,  -15.,    0.],
        [  16.,  -48.,    0.],
        [  23.,  -42.,    0.],
        [  27.,  -22.,    0.],
        [  26.,  -11.,    0.],
        [  30.,   -8.,    0.],
        [  22.,   -2.,    0.],
        [  10.,    0.,    0.],
        [   1.,    5.,    0.],
        [ -53.,   26.,    0.],
        [ -36.,   44.,    0.],
        [ -16.,   35.,    0.],
        [ -43.,   66.,    0.],
        [ -51.,   34.,    0.],
   ... (truncated)

                CALL: <listcomp> in ['C:\\Users\\Brandon\\Documents\\00 Programs 00\\', 'sketchrnn-pytorch\\sketch_rnn\\dataset.py']:99
                Arguments: .0=<iterator object at 0x000002DD5D162D10>

                RETURN: [tensor([[-18.,   1.,   0.],
        [ -7.,   5.,   0.],
        [ -9.,   7.,   0.],
        [-10.,  19.,   0.],
        [ -1.,  13.,   0.],
        [  4.,   5.,   0.],
        [ 32.,   1.,   0.],
        [ 22., -10.,   0.],
        [  3., -21.,   0.],
        [-10., -11.,   0.],
        [-17.,  -7.,   1.],
        [-25., -21.,   0.],
        [ 11.,  20.,   1.],
        [ 32., -22.,   0.],
        [ -7.,  19.,   1.],
        [-29.,  26.,   0.],
        [  0.,   0.,   1.],
        [ 30.,  -1.,   ... (truncated)

            RETURN: None

            CALL: normalize in sketchrnn-pytorch\sketch_rnn\dataset.py:108
            Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD5D1D31F0>, scale_factor=Tensor(shape=(), dtype=torch.float32, device=cpu)

            RETURN: None

        RETURN: None

        CALL: __init__ in sketchrnn-pytorch\train_sketch_rnn.py:52
        Arguments: self=<train_sketch_rnn.CollateFn object at 0x000002DD5D162D10>, max_seq_len=129

        RETURN: None

    CALL: __len__ in sketchrnn-pytorch\sketch_rnn\dataset.py:116
    Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>

    RETURN: 70000

    CALL: __len__ in sketchrnn-pytorch\sketch_rnn\dataset.py:116
    Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>

    RETURN: 70000

        CALL: __init__ in sketchrnn-pytorch\sketch_rnn\model.py:48
        Arguments: self=<Unprintable SketchRNN>, hps=Namespace(max_seq_len=129, enc_model='lstm', dec_model='layer_norm', enc_rnn_size=256, dec_rnn_size=512, z_size=128, num_mixture=20, r_dropout=0.1, kl_weight=0.5, kl_weight_start=0.01, kl_tolerance=0.2, kl_decay_rate=0.99995, reg_covar=1e-06, batch_size=100, lr=0.001, lr_decay=0.9999, min_lr=1e-05, grad_clip=1.0, data_set='cat.npz', random_scale_factor=0.15, augment_stroke_prob=0.1, data_dir='data/', save_dir=None, num_epochs=4, num_workers=8)

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\model.py:19
            Arguments: self=<Unprintable Encoder>, hidden_size=256, z_size=128

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\model.py:26
                Arguments: self=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
)

                    CALL: init_orthogonal_ in sketchrnn-pytorch\sketch_rnn\rnn.py:12
                    Arguments: weight=Tensor(shape=(1024, 256), dtype=torch.float32, device=cpu), hsize=256

                    RETURN: None

                    CALL: init_orthogonal_ in sketchrnn-pytorch\sketch_rnn\rnn.py:12
                    Arguments: weight=Tensor(shape=(1024, 256), dtype=torch.float32, device=cpu), hsize=256

                    RETURN: None

                RETURN: None

            RETURN: None

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\rnn.py:97
            Arguments: self=<Unprintable LayerNormLSTMCell>, input_size=133, hidden_size=512, forget_bias=1.0, r_dropout=0.1

                CALL: __init__ in sketchrnn-pytorch\sketch_rnn\rnn.py:71
                Arguments: self=<Unprintable ChunkLayerNorm>, num_units=512, chunks=4, eps=1e-05, affine=True

                    CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:82
                    Arguments: self=ChunkLayerNorm()

                    RETURN: None

                RETURN: None

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:113
                Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

                    CALL: init_orthogonal_ in sketchrnn-pytorch\sketch_rnn\rnn.py:12
                    Arguments: weight=Tensor(shape=(2048, 512), dtype=torch.float32, device=cpu), hsize=512

                    RETURN: None

                    CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:82
                    Arguments: self=ChunkLayerNorm()

                    RETURN: None

                RETURN: None

            RETURN: None

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\rnn.py:334
            Arguments: self=<Unprintable LSTMLayer>, cell=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
), batch_first=True, reverse=False

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:344
                Arguments: self=LSTMLayer(
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)

                    CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:113
                    Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

                        CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:82
                        Arguments: self=ChunkLayerNorm()

                        RETURN: None

                    RETURN: None

                RETURN: None

            RETURN: None

    CALL: state_size in sketchrnn-pytorch\sketch_rnn\rnn.py:119
    Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

    RETURN: 1024

    CALL: state_size in sketchrnn-pytorch\sketch_rnn\rnn.py:119
    Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

    RETURN: 1024

    CALL: state_size in sketchrnn-pytorch\sketch_rnn\rnn.py:119
    Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

    RETURN: 1024

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\param_layer.py:10
            Arguments: self=<Unprintable ParameterLayer>, input_size=512, k=20, d=2

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\param_layer.py:16
                Arguments: self=ParameterLayer(
  (linear): Linear(in_features=512, out_features=123, bias=True)
)

                RETURN: None

            RETURN: None

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\objective.py:30
            Arguments: self=<Unprintable KLLoss>, kl_weight=0.5, eta_min=0.01, R=0.99995, kl_min=0.2

            RETURN: None

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\objective.py:76
            Arguments: self=<Unprintable DrawingLoss>, reg_covar=1e-06

            RETURN: None

            CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\model.py:75
            Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated)

                CALL: <lambda> in sketchrnn-pytorch\sketch_rnn\model.py:76
                Arguments: m=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
)

                RETURN: True

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\model.py:26
                Arguments: self=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
)

                RETURN: None

                CALL: <lambda> in sketchrnn-pytorch\sketch_rnn\model.py:76
                Arguments: m=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

                RETURN: True

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\rnn.py:113
                Arguments: self=LayerNormLSTMCell(
  (r_dropout): Dropout(p=0.1, inplace=False)
  (layernorm_h): ChunkLayerNorm()
  (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

                RETURN: None

                CALL: <lambda> in sketchrnn-pytorch\sketch_rnn\model.py:76
                Arguments: m=RecursiveScriptModule(
  original_name=LSTMLayer
  (cell): RecursiveScriptModule(
    original_name=LayerNormLSTMCell
    (r_dropout): RecursiveScriptModule(original_name=Dropout)
    (layernorm_h): RecursiveScriptModule(original_name=ChunkLayerNorm)
    (layernorm_c): RecursiveScriptModule(original_name=LayerNorm)
  )
)

                RETURN: False

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\param_layer.py:16
                Arguments: self=ParameterLayer(
  (linear): Linear(in_features=512, out_features=123, bias=True)
)

                RETURN: None

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\objective.py:38
                Arguments: self=KLLoss()

                RETURN: None

                CALL: reset_parameters in sketchrnn-pytorch\sketch_rnn\objective.py:80
                Arguments: self=DrawingLoss()

                RETURN: None

            RETURN: None

        RETURN: None

        CALL: train_epoch in sketchrnn-pytorch\train_sketch_rnn.py:16
        Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D26E0>, optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0
), scheduler=<torch.optim.lr_scheduler.ExponentialLR object at 0x000002DD601C2650>, device=device(type='cuda'), grad_clip=1.0

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\utils\training.py:13
            Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C3730>

            RETURN: None

            CALL: __len__ in sketchrnn-pytorch\sketch_rnn\dataset.py:116
            Arguments: self=<sketch_rnn.dataset.SketchRNNDataset object at 0x000002DD601C2290>

            RETURN: 70000

            CALL: model_step in sketchrnn-pytorch\sketch_rnn\model.py:110
            Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:102
    Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

        CALL: _forward in sketchrnn-pytorch\sketch_rnn\model.py:83
        Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), enc_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), dec_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), enc_lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:36
    Arguments: self=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
), x=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    RETURN: (tensor([[ 0.1641,  2.2238,  1.4203,  ..., -0.1354,  0.8250,  1.6512],
        [ 1.1084, -1.5421,  0.9014,  ...,  1.1165, -1.2045, -0.0631],
        [ 0.3338, -0.2434,  0.1484,  ...,  0.7497,  0.4243,  3.7498],
        ...,
        [-0.2568,  0.0114, -1.8626,  ..., -0.4895,  0.1227, -0.1910],
        [ 1.1833, -0.1817,  0.3278,  ..., -0.2832,  1.4953, -0.9462],
        [-0.3146, -0.8072,  1.8464,  ..., -1.0178,  0.1638,  0.4259]],
       device='cuda:0', grad_fn=<AddBackward0>), tensor([[-3.0650... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\param_layer.py:19
    Arguments: self=ParameterLayer(
  (linear): Linear(in_features=512, out_features=123, bias=True)
), x=Tensor(shape=(100, 129, 512), dtype=torch.float32, device=cuda:0), T=1

    RETURN: (tensor([[[-3.1450, -3.0181, -3.2161,  ..., -3.1853, -2.6969, -2.8842],
         [-3.2769, -2.8322, -3.0838,  ..., -3.1750, -2.8571, -2.9095],
         [-3.3520, -2.7254, -3.1868,  ..., -3.0982, -2.8880, -3.0380],
         ...,
         [-3.2161, -2.7814, -3.0289,  ..., -2.9985, -2.9288, -3.0588],
         [-3.2124, -2.7697, -3.0445,  ..., -2.9968, -2.9306, -3.0764],
         [-3.2183, -2.7729, -3.0498,  ..., -3.0087, -2.9217, -3.0795]],

        [[-2.9116, -2.9656, -2.6988,  ..., -2.8344, -2.80... (truncated)

        RETURN: ((tensor([[[-3.1450, -3.0181, -3.2161,  ..., -3.1853, -2.6969, -2.8842],
         [-3.2769, -2.8322, -3.0838,  ..., -3.1750, -2.8571, -2.9095],
         [-3.3520, -2.7254, -3.1868,  ..., -3.0982, -2.8880, -3.0380],
         ...,
         [-3.2161, -2.7814, -3.0289,  ..., -2.9985, -2.9288, -3.0588],
         [-3.2124, -2.7697, -3.0445,  ..., -2.9968, -2.9306, -3.0764],
         [-3.2183, -2.7729, -3.0498,  ..., -3.0087, -2.9217, -3.0795]],

        [[-2.9116, -2.9656, -2.6988,  ..., -2.8344, -2.8... (truncated)

    RETURN: ((tensor([[[-3.1450, -3.0181, -3.2161,  ..., -3.1853, -2.6969, -2.8842],
         [-3.2769, -2.8322, -3.0838,  ..., -3.1750, -2.8571, -2.9095],
         [-3.3520, -2.7254, -3.1868,  ..., -3.0982, -2.8880, -3.0380],
         ...,
         [-3.2161, -2.7814, -3.0289,  ..., -2.9985, -2.9288, -3.0588],
         [-3.2124, -2.7697, -3.0445,  ..., -2.9968, -2.9306, -3.0764],
         [-3.2183, -2.7729, -3.0498,  ..., -3.0087, -2.9217, -3.0795]],

        [[-2.9116, -2.9656, -2.6988,  ..., -2.8344, -2.8... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:49
    Arguments: self=KLLoss(), q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

        CALL: kl_divergence in sketchrnn-pytorch\sketch_rnn\objective.py:19
        Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

            CALL: kl_divergence_sn_prior in sketchrnn-pytorch\sketch_rnn\objective.py:14
            Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        CALL: weight in sketchrnn-pytorch\sketch_rnn\objective.py:41
        Arguments: self=KLLoss()

        RETURN: 0.004999995231628418

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:83
    Arguments: self=DrawingLoss(), x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), v=Tensor(shape=(100, 129), dtype=torch.int64, device=cuda:0), params=(tensor([[[-3.1450, -3.0181, -3.2161,  ..., -3.1853, -2.6969, -2.8842],
         [-3.2769, -2.8322, -3.0838,  ..., -3.1750, -2.8571, -2.9095],
         [-3.3520, -2.7254, -3.1868,  ..., -3.0982, -2.8880, -3.0380],
         ...,
         [-3.2161, -2.7814, -3.0289,  ..., -2.9985, -2.9288, -3.0588],
         [-3.2124, -2.7697, -3.0445,  ..., -2.9968, -2.9306, -3.0764],
         [-3.2183, -2.7729, -3.0498,  ..., -3.0087, -2.9217, -3.0795]],

        [[-2.9116, -2.9656, -2.6988,  ..., -2.8344, -2.80... (truncated)

        CALL: tikhonov_reg2d in sketchrnn-pytorch\sketch_rnn\utils\gmm.py:7
        Arguments: scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0), alpha=1e-06

        RETURN: (tensor([[[[1.3437, 1.3621],
          [1.1997, 0.7544],
          [1.3173, 0.9872],
          ...,
          [0.7522, 0.9772],
          [1.2621, 0.7662],
          [1.3020, 0.8141]],

         [[1.3339, 1.3959],
          [1.3476, 0.8821],
          [1.4696, 0.9008],
          ...,
          [0.8472, 1.0194],
          [1.1811, 0.8766],
          [1.2226, 0.9811]],

         [[1.2959, 1.3571],
          [1.5321, 1.0487],
          [1.7236, 0.9086],
          ...,
          [0.8754, 0.9459],
  ... (truncated)

        CALL: mvn_log_prob in sketchrnn-pytorch\sketch_rnn\objective.py:60
        Arguments: x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), means=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            CALL: update in sketchrnn-pytorch\sketch_rnn\utils\training.py:25
            Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C3730>, val=3.877295970916748, n=100

            RETURN: None

            CALL: model_step in sketchrnn-pytorch\sketch_rnn\model.py:110
            Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:102
    Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

        CALL: _forward in sketchrnn-pytorch\sketch_rnn\model.py:83
        Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), enc_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), dec_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), enc_lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:36
    Arguments: self=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
), x=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    RETURN: (tensor([[ 0.2559,  1.1294,  0.1155,  ...,  0.2193, -0.3356,  0.1107],
        [-0.8761, -0.0771, -1.2932,  ..., -0.6456, -1.3335, -0.5667],
        [-0.3400, -0.6553,  1.1316,  ..., -0.0057,  0.1000, -0.3917],
        ...,
        [ 0.3161, -0.4419,  0.5327,  ..., -1.7563, -0.6062, -0.1020],
        [-1.4426, -1.2943,  0.5138,  ...,  1.8757,  0.2728,  1.4374],
        [ 2.1394,  0.2845,  0.5518,  ...,  1.9226,  2.2235,  0.2086]],
       device='cuda:0', grad_fn=<AddBackward0>), tensor([[ 0.0009... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\param_layer.py:19
    Arguments: self=ParameterLayer(
  (linear): Linear(in_features=512, out_features=123, bias=True)
), x=Tensor(shape=(100, 129, 512), dtype=torch.float32, device=cuda:0), T=1

    RETURN: (tensor([[[-3.3003, -2.9167, -3.0546,  ..., -3.0204, -2.9697, -2.9834],
         [-3.0670, -2.9857, -3.1842,  ..., -3.1105, -2.9149, -3.1167],
         [-3.1249, -3.0053, -3.2030,  ..., -3.0469, -2.8879, -3.1508],
         ...,
         [-3.1253, -3.0391, -3.0995,  ..., -3.0132, -2.9762, -3.1368],
         [-3.1276, -3.0522, -3.1160,  ..., -3.0044, -2.9715, -3.1224],
         [-3.1152, -3.0643, -3.1160,  ..., -3.0068, -2.9727, -3.1200]],

        [[-3.3309, -2.8823, -3.2869,  ..., -2.8337, -2.95... (truncated)

        RETURN: ((tensor([[[-3.3003, -2.9167, -3.0546,  ..., -3.0204, -2.9697, -2.9834],
         [-3.0670, -2.9857, -3.1842,  ..., -3.1105, -2.9149, -3.1167],
         [-3.1249, -3.0053, -3.2030,  ..., -3.0469, -2.8879, -3.1508],
         ...,
         [-3.1253, -3.0391, -3.0995,  ..., -3.0132, -2.9762, -3.1368],
         [-3.1276, -3.0522, -3.1160,  ..., -3.0044, -2.9715, -3.1224],
         [-3.1152, -3.0643, -3.1160,  ..., -3.0068, -2.9727, -3.1200]],

        [[-3.3309, -2.8823, -3.2869,  ..., -2.8337, -2.9... (truncated)

    RETURN: ((tensor([[[-3.3003, -2.9167, -3.0546,  ..., -3.0204, -2.9697, -2.9834],
         [-3.0670, -2.9857, -3.1842,  ..., -3.1105, -2.9149, -3.1167],
         [-3.1249, -3.0053, -3.2030,  ..., -3.0469, -2.8879, -3.1508],
         ...,
         [-3.1253, -3.0391, -3.0995,  ..., -3.0132, -2.9762, -3.1368],
         [-3.1276, -3.0522, -3.1160,  ..., -3.0044, -2.9715, -3.1224],
         [-3.1152, -3.0643, -3.1160,  ..., -3.0068, -2.9727, -3.1200]],

        [[-3.3309, -2.8823, -3.2869,  ..., -2.8337, -2.9... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:49
    Arguments: self=KLLoss(), q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

        CALL: kl_divergence in sketchrnn-pytorch\sketch_rnn\objective.py:19
        Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

            CALL: kl_divergence_sn_prior in sketchrnn-pytorch\sketch_rnn\objective.py:14
            Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        CALL: weight in sketchrnn-pytorch\sketch_rnn\objective.py:41
        Arguments: self=KLLoss()

        RETURN: 0.005024760961532593

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:83
    Arguments: self=DrawingLoss(), x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), v=Tensor(shape=(100, 129), dtype=torch.int64, device=cuda:0), params=(tensor([[[-3.3003, -2.9167, -3.0546,  ..., -3.0204, -2.9697, -2.9834],
         [-3.0670, -2.9857, -3.1842,  ..., -3.1105, -2.9149, -3.1167],
         [-3.1249, -3.0053, -3.2030,  ..., -3.0469, -2.8879, -3.1508],
         ...,
         [-3.1253, -3.0391, -3.0995,  ..., -3.0132, -2.9762, -3.1368],
         [-3.1276, -3.0522, -3.1160,  ..., -3.0044, -2.9715, -3.1224],
         [-3.1152, -3.0643, -3.1160,  ..., -3.0068, -2.9727, -3.1200]],

        [[-3.3309, -2.8823, -3.2869,  ..., -2.8337, -2.95... (truncated)

        CALL: tikhonov_reg2d in sketchrnn-pytorch\sketch_rnn\utils\gmm.py:7
        Arguments: scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0), alpha=1e-06

        RETURN: (tensor([[[[0.8731, 1.0467],
          [1.0542, 0.7451],
          [1.7055, 1.0837],
          ...,
          [1.1326, 0.8850],
          [0.7147, 1.0329],
          [0.6250, 1.1045]],

         [[0.9900, 0.9288],
          [1.2237, 0.7256],
          [1.6162, 1.0356],
          ...,
          [1.1224, 0.9564],
          [0.7617, 0.9511],
          [0.6408, 0.9573]],

         [[0.9684, 0.9775],
          [1.1233, 0.8078],
          [1.6520, 1.1198],
          ...,
          [1.0993, 1.0085],
  ... (truncated)

        CALL: mvn_log_prob in sketchrnn-pytorch\sketch_rnn\objective.py:60
        Arguments: x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), means=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            CALL: update in sketchrnn-pytorch\sketch_rnn\utils\training.py:25
            Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C3730>, val=3.8215229511260986, n=100

            RETURN: None

            CALL: model_step in sketchrnn-pytorch\sketch_rnn\model.py:110
            Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:102
    Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data=Tensor(shape=(100, 130, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

        CALL: _forward in sketchrnn-pytorch\sketch_rnn\model.py:83
        Arguments: self=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), enc_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), dec_inputs=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), enc_lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\model.py:36
    Arguments: self=Encoder(
  (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
  (output): Linear(in_features=512, out_features=256, bias=True)
), x=Tensor(shape=(100, 129, 5), dtype=torch.float32, device=cuda:0), lengths=Tensor(shape=(100,), dtype=torch.int64, device=cuda:0)

    RETURN: (tensor([[-1.6742,  0.7852, -0.9218,  ...,  1.0848, -0.0316, -1.2851],
        [ 0.5108, -0.1074, -0.7616,  ..., -1.6327,  1.0907, -0.7731],
        [ 0.4049, -0.1718,  0.5631,  ..., -0.0545,  0.9775, -1.8523],
        ...,
        [ 0.6622,  0.4034,  1.0763,  ..., -0.3753, -0.9719,  0.1385],
        [-0.4629, -0.8654, -1.2321,  ...,  0.4501,  1.5339,  0.1824],
        [-0.7899,  0.6735, -0.6577,  ...,  1.1523, -0.5011,  0.7165]],
       device='cuda:0', grad_fn=<AddBackward0>), tensor([[ 0.0041... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\param_layer.py:19
    Arguments: self=ParameterLayer(
  (linear): Linear(in_features=512, out_features=123, bias=True)
), x=Tensor(shape=(100, 129, 512), dtype=torch.float32, device=cuda:0), T=1

    RETURN: (tensor([[[-2.8538, -3.0608, -2.9674,  ..., -3.2595, -2.7133, -2.9116],
         [-3.0521, -3.0395, -2.8545,  ..., -3.3068, -2.6795, -2.8198],
         [-3.0894, -2.9256, -2.8312,  ..., -3.3420, -2.7505, -2.8302],
         ...,
         [-3.1937, -3.0359, -2.8453,  ..., -3.2129, -2.9024, -2.8973],
         [-3.2122, -3.0284, -2.8452,  ..., -3.2086, -2.9140, -2.8883],
         [-3.2016, -3.0273, -2.8508,  ..., -3.2062, -2.8941, -2.9126]],

        [[-3.4213, -2.8891, -2.6373,  ..., -2.7117, -3.02... (truncated)

        RETURN: ((tensor([[[-2.8538, -3.0608, -2.9674,  ..., -3.2595, -2.7133, -2.9116],
         [-3.0521, -3.0395, -2.8545,  ..., -3.3068, -2.6795, -2.8198],
         [-3.0894, -2.9256, -2.8312,  ..., -3.3420, -2.7505, -2.8302],
         ...,
         [-3.1937, -3.0359, -2.8453,  ..., -3.2129, -2.9024, -2.8973],
         [-3.2122, -3.0284, -2.8452,  ..., -3.2086, -2.9140, -2.8883],
         [-3.2016, -3.0273, -2.8508,  ..., -3.2062, -2.8941, -2.9126]],

        [[-3.4213, -2.8891, -2.6373,  ..., -2.7117, -3.0... (truncated)

    RETURN: ((tensor([[[-2.8538, -3.0608, -2.9674,  ..., -3.2595, -2.7133, -2.9116],
         [-3.0521, -3.0395, -2.8545,  ..., -3.3068, -2.6795, -2.8198],
         [-3.0894, -2.9256, -2.8312,  ..., -3.3420, -2.7505, -2.8302],
         ...,
         [-3.1937, -3.0359, -2.8453,  ..., -3.2129, -2.9024, -2.8973],
         [-3.2122, -3.0284, -2.8452,  ..., -3.2086, -2.9140, -2.8883],
         [-3.2016, -3.0273, -2.8508,  ..., -3.2062, -2.8941, -2.9126]],

        [[-3.4213, -2.8891, -2.6373,  ..., -2.7117, -3.0... (truncated)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:49
    Arguments: self=KLLoss(), q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

        CALL: kl_divergence in sketchrnn-pytorch\sketch_rnn\objective.py:19
        Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), p_mean=None, p_logvar=None

            CALL: kl_divergence_sn_prior in sketchrnn-pytorch\sketch_rnn\objective.py:14
            Arguments: q_mean=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0), q_logvar=Tensor(shape=(100, 128), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

        CALL: weight in sketchrnn-pytorch\sketch_rnn\objective.py:41
        Arguments: self=KLLoss()

        RETURN: 0.005049526691436768

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

    CALL: forward in sketchrnn-pytorch\sketch_rnn\objective.py:83
    Arguments: self=DrawingLoss(), x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), v=Tensor(shape=(100, 129), dtype=torch.int64, device=cuda:0), params=(tensor([[[-2.8538, -3.0608, -2.9674,  ..., -3.2595, -2.7133, -2.9116],
         [-3.0521, -3.0395, -2.8545,  ..., -3.3068, -2.6795, -2.8198],
         [-3.0894, -2.9256, -2.8312,  ..., -3.3420, -2.7505, -2.8302],
         ...,
         [-3.1937, -3.0359, -2.8453,  ..., -3.2129, -2.9024, -2.8973],
         [-3.2122, -3.0284, -2.8452,  ..., -3.2086, -2.9140, -2.8883],
         [-3.2016, -3.0273, -2.8508,  ..., -3.2062, -2.8941, -2.9126]],

        [[-3.4213, -2.8891, -2.6373,  ..., -2.7117, -3.02... (truncated)

        CALL: tikhonov_reg2d in sketchrnn-pytorch\sketch_rnn\utils\gmm.py:7
        Arguments: scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0), alpha=1e-06

        RETURN: (tensor([[[[1.0144, 1.0052],
          [1.0710, 1.1147],
          [0.7971, 1.0167],
          ...,
          [0.7774, 0.8751],
          [0.6155, 1.3787],
          [1.2576, 1.1378]],

         [[1.3415, 0.9861],
          [1.0706, 1.1221],
          [0.5764, 0.8220],
          ...,
          [0.7628, 0.9203],
          [0.5938, 1.3765],
          [1.1049, 0.9821]],

         [[1.3260, 0.9140],
          [1.0017, 1.0123],
          [0.6175, 0.8856],
          ...,
          [0.7034, 0.9770],
  ... (truncated)

        CALL: mvn_log_prob in sketchrnn-pytorch\sketch_rnn\objective.py:60
        Arguments: x=Tensor(shape=(100, 129, 2), dtype=torch.float32, device=cuda:0), means=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), scales=Tensor(shape=(100, 129, 20, 2), dtype=torch.float32, device=cuda:0), corrs=Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

        RETURN: Tensor(shape=(100, 129, 20), dtype=torch.float32, device=cuda:0)

    RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            RETURN: Tensor(shape=(), dtype=torch.float32, device=cuda:0)

            CALL: update in sketchrnn-pytorch\sketch_rnn\utils\training.py:25
            Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C3730>, val=3.8320093154907227, n=100

            RETURN: None

        RETURN: 1.363250998514039

    CALL: eval_epoch in sketchrnn-pytorch\train_sketch_rnn.py:40
    Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D3130>, device=device(type='cuda')

        CALL: __init__ in sketchrnn-pytorch\sketch_rnn\utils\training.py:13
        Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C1F60>

        RETURN: None

    RETURN: 0.7109472894668579

        CALL: train_epoch in sketchrnn-pytorch\train_sketch_rnn.py:16
        Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D26E0>, optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.000932390556315722
    maximize: False
    weight_decay: 0
), scheduler=<torch.optim.lr_scheduler.ExponentialLR object at 0x000002DD601C2650>, device=device(type='cuda'), grad_clip=1.0

            CALL: __init__ in sketchrnn-pytorch\sketch_rnn\utils\training.py:13
            Arguments: self=<sketch_rnn.utils.training.AverageMeter object at 0x000002DD601C33D0>

            RETURN: None

        RETURN: 0.6998481866291592

    CALL: eval_epoch in sketchrnn-pytorch\train_sketch_rnn.py:40
    Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D3130>, device=device(type='cuda')

    RETURN: 0.45709126085042956

        CALL: train_epoch in sketchrnn-pytorch\train_sketch_rnn.py:16
        Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D26E0>, optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0008693521495067418
    maximize: False
    weight_decay: 0
), scheduler=<torch.optim.lr_scheduler.ExponentialLR object at 0x000002DD601C2650>, device=device(type='cuda'), grad_clip=1.0

        RETURN: 0.5354434898921422

    CALL: eval_epoch in sketchrnn-pytorch\train_sketch_rnn.py:40
    Arguments: model=SketchRNN(
  (encoder): Encoder(
    (rnn): LSTM(5, 256, batch_first=True, bidirectional=True)
    (output): Linear(in_features=512, out_features=256, bias=True)
  )
  (cell): LayerNormLSTMCell(
    (r_dropout): Dropout(p=0.1, inplace=False)
    (layernorm_h): ChunkLayerNorm()
    (layernorm_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): RecursiveScriptModule(
    original_name=LSTMLayer
    (cell): RecursiveScriptModule(
      original_name=LayerNormLSTMCell
      (r... (truncated), data_loader=<torch.utils.data.dataloader.DataLoader object at 0x000002DD5D1D3130>, device=device(type='cuda')

    RETURN: 0.32151466876268386

    RETURN: None

